{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cfc1267-442f-4c79-aad2-bdc706fbf4f5",
   "metadata": {},
   "source": [
    "# Gun Detection System for Urban Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc608c4-6cf4-4953-9285-6cfa024b43e9",
   "metadata": {},
   "source": [
    "This notebook contains the implementation of a **gun detection system** for the detection of gunshots sounds in urban settings. The system is designed to detect gunshots by analysing audio data and uses machine learning algorithms to classify the sounds. The proposed system has several potential applications, including the ability to provide real-time information to law enforcement agencies about the location and nature of gunshots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fc624a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "matplotlib.use(\"Agg\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e5c5f-e49a-44bb-bc3b-537770b74e71",
   "metadata": {},
   "source": [
    "First, some constant values are set to be used across the entire notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6183ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAMPLE_RATE = 22050 # Justification for this value is provided later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece1f1ca",
   "metadata": {},
   "source": [
    "## Training, validation & testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ed2c7-9c0f-42ca-9d92-678b130bae55",
   "metadata": {},
   "source": [
    "In this section both proposed approaches will be implemented using different models for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906280b-7e8d-41d7-a146-1cfda3861307",
   "metadata": {},
   "source": [
    "***For both of the following approaches, the training data is split equally into ten classes, with only one of them being the target class (gun_shot). Treating this as a binary classification problem by considering only one class as true and all others as false might not result in good classification accuracy, as the correct class will be underrepresented. Therefore, it is better to treat this as a normal classification problem with ten categories. After the classifier predicts the class, the output will be checked for whether it is a gunshot or not. If the predicted class is a gunshot, then it will be considered true; otherwise, it will be considered false.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6895bc-89e0-4d7e-8428-b8b3905b3dc5",
   "metadata": {},
   "source": [
    "Tensorflow 2.11 will be used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b528a9-fce7-4fe1-a486-67646accf95e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.11.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (2.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (58.0.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (0.3.3)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (3.19.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (1.21.6)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (1.42.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (4.5.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (16.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (0.32.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (1.13.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (1.4.0)\n",
      "Requirement already satisfied: packaging in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (21.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorflow==2.11.0) (2.11.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.33.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages (from packaging->tensorflow==2.11.0) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e315fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 21:34:33.904606: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-19 21:34:34.184517: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-19 21:34:34.448417: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732052074.677362    4709 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732052074.747109    4709 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-19 21:34:35.286710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e1f70-ab2b-4cd2-a993-68b83b48844d",
   "metadata": {},
   "source": [
    "The following functions will be used for the evaluation and comparison of the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4ffdfe8-130c-4554-b311-e0a375d2e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def _sensitivity(y_actual, y_pred):\n",
    "    \"\"\"Calculate the sensitivity score per class for a model\"\"\"\n",
    "    cm = confusion_matrix(y_actual, y_pred)\n",
    "    FN = cm[1, 0]\n",
    "    TP = cm[1, 1]\n",
    "    \n",
    "    if(not TP): # avoid Nan values if both TP and FN are 0s\n",
    "        sensitivity = 0.0\n",
    "    else:\n",
    "        sensitivity = (TP / (TP + FN)).round(2)\n",
    "        \n",
    "    return sensitivity\n",
    "\n",
    "def _specificity(y_actual, y_pred):\n",
    "    \"\"\"Calculate the specificity score per class for a model\"\"\"\n",
    "    cm = confusion_matrix(y_actual, y_pred)\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    \n",
    "    if(not TN): # avoid Nan values if both TN and FP are 0s\n",
    "        specificity = 0.0\n",
    "    else:\n",
    "        specificity = (TN / (TN + FP)).round(2)\n",
    "        \n",
    "    return specificity\n",
    "\n",
    "def report(y_actual, y_pred):\n",
    "    \"\"\"Print a report with all evaluation metrics for a model\"\"\"\n",
    "    print(classification_report(y_actual, y_pred))\n",
    "    sensitivity = _sensitivity(y_actual, y_pred)\n",
    "    print('Sensitivity: ', sensitivity)\n",
    "    specificity = _specificity(y_actual, y_pred)\n",
    "    print('Specificity: ', specificity)\n",
    "    \n",
    "    cm_matrix = confusion_matrix(y_actual,y_pred)\n",
    "    sns.heatmap(cm_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71310969-67c3-41c0-9df7-dd3225c14cf2",
   "metadata": {},
   "source": [
    "### Approach I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91017f65-8eb6-40f1-b00f-5be5f896e721",
   "metadata": {},
   "source": [
    "This approach involves the classification of mel-spectrogram images representing the audio slices. This moves this task from audio classification into the image classification paradigm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fedde71-18d6-4851-bb9d-944bb9e8a3b6",
   "metadata": {},
   "source": [
    "For this approach, a 2D deep convolutional neural network (CNN) will be trained on the extracted mel-spectrogram images. To achive this TensorFlow with Keras will be used to construct, train, validate and test the CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54880b3-75e6-4619-a1d8-b6ca2c7d089b",
   "metadata": {},
   "source": [
    "The training, validation and test sets will be loaded from each respective images folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8e9b23-ed3c-4111-85b3-a94c02786719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7000 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "'''The training images set'''\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    IMG_DIR,\n",
    "    labels=\"inferred\",\n",
    "    batch_size=1,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03cff9a2-2daa-4026-8c87-65ad49e05026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 812 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "'''The validation images set'''\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    VAL_DIR,\n",
    "    labels=\"inferred\",\n",
    "    batch_size=1,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d08d2108-8ea5-424e-841a-5b0846db49b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 902 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "'''The testing images set'''\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    TEST_DIR,\n",
    "    labels=\"inferred\",\n",
    "    batch_size=1,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea476a68-58af-4594-9675-a8e5d2a5ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_spectrograms, example_spect_labels in train_ds.take(1):\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac30fa-9d1f-4d30-a711-c3679eb459ca",
   "metadata": {},
   "source": [
    "The following CNN structure will be used. The CNN takes as an input a 3 channel RGB images. The input layer takes in the image in the original size, then the following layer resizes the images into 32 x 32. Resizing the image reduces the computational complexity of the model. Then, the next layer applies normalization to the image to standardize it. The output then passes through 2 convolutional layers with 16 and 32 filters, respectively. The activation function used is `ReLU`. A max pooling layer then performs a 2x2 pooling operation to reduce the spatial dimensions of the output followed by a dropout layer that randomly drops out 25% of the connections to prevent overfitting. The output is flattened and inputed into a dense layer with 128 nodes. The output goes through another dropout layer with 50% before reaching the last dense layer with ten nodes representing the 10 output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2169c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/msc1/anaconda3/envs/Env-7146COMP/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "input_shape = example_spectrograms.shape[1:]\n",
    "num_classes = 10\n",
    "\n",
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(data=train_ds.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=input_shape),\n",
    "    tf.keras.layers.Resizing(32, 32),\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Conv2D(16, 3, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a0e12-86d3-4666-a181-b8ca29ce4469",
   "metadata": {},
   "source": [
    "<br/> The model is compiled using adam optimizer and accuracy as evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a13e50-dbdb-4e0b-9c1f-14350929f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130d8da-3770-4eef-9f77-6618cbcc7d02",
   "metadata": {},
   "source": [
    "The model is trained with 15 epochs. The validation split will be used for fine tunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d824e94-1adf-4196-9898-997f78930e42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 01:42:33.761602: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-04-07 01:42:34.214479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-04-07 01:42:35.084285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-04-07 01:42:35.085275: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fbebe264f00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-07 01:42:35.085285: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-04-07 01:42:35.087933: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-07 01:42:35.176628: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 23s 3ms/step - loss: 1.5077 - accuracy: 0.4991 - val_loss: 1.0758 - val_accuracy: 0.6330\n",
      "Epoch 2/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 1.0686 - accuracy: 0.6484 - val_loss: 1.0033 - val_accuracy: 0.6810\n",
      "Epoch 3/15\n",
      "7000/7000 [==============================] - 20s 3ms/step - loss: 0.8873 - accuracy: 0.7103 - val_loss: 0.8485 - val_accuracy: 0.7229\n",
      "Epoch 4/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.7513 - accuracy: 0.7537 - val_loss: 0.8775 - val_accuracy: 0.7217\n",
      "Epoch 5/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.6499 - accuracy: 0.7900 - val_loss: 1.0231 - val_accuracy: 0.7241\n",
      "Epoch 6/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.5874 - accuracy: 0.8084 - val_loss: 0.9869 - val_accuracy: 0.7414\n",
      "Epoch 7/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.5593 - accuracy: 0.8257 - val_loss: 0.9460 - val_accuracy: 0.7438\n",
      "Epoch 8/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.5223 - accuracy: 0.8314 - val_loss: 1.0155 - val_accuracy: 0.7438\n",
      "Epoch 9/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.5048 - accuracy: 0.8451 - val_loss: 0.9818 - val_accuracy: 0.7562\n",
      "Epoch 10/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.4581 - accuracy: 0.8557 - val_loss: 1.0949 - val_accuracy: 0.7537\n",
      "Epoch 11/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.4641 - accuracy: 0.8579 - val_loss: 0.9873 - val_accuracy: 0.7451\n",
      "Epoch 12/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.4779 - accuracy: 0.8516 - val_loss: 1.1526 - val_accuracy: 0.7488\n",
      "Epoch 13/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.4326 - accuracy: 0.8716 - val_loss: 1.1234 - val_accuracy: 0.7451\n",
      "Epoch 14/15\n",
      "7000/7000 [==============================] - 21s 3ms/step - loss: 0.4362 - accuracy: 0.8689 - val_loss: 1.4347 - val_accuracy: 0.7475\n",
      "Epoch 15/15\n",
      "7000/7000 [==============================] - 20s 3ms/step - loss: 0.4227 - accuracy: 0.8733 - val_loss: 1.0884 - val_accuracy: 0.7525\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=15\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a710c-f31f-46c2-921e-416beabdf684",
   "metadata": {},
   "source": [
    "<br/> While training the model, the highest validation accuracy achieved was 75% reached on the 9th epoch with a loss of 0.98. Afterwards, the validation accuracy remained almost the same. The average accuracy of the last 5 epochs was **74.78%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d0893-a7ab-44aa-9b0a-3746f5a0bb29",
   "metadata": {},
   "source": [
    "To get the true accuracy of the model, it is tested against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cead682-1e3f-4d69-a365-6937d0ad2458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902/902 [==============================] - 1s 963us/step - loss: 1.1927 - accuracy: 0.7328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.1927478313446045, 'accuracy': 0.7328159809112549}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82ed21-e544-456f-8d51-913180139e47",
   "metadata": {},
   "source": [
    "The model scored an accuracy of 73.28% on the test which is an adequate accuracy. However, this value reflects the accuracy of the model across all classes. For the current task, the model should be evaluated on its ability to differentiate between gunshot sounds and all other noises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ac6fd-f6d2-41b4-b091-97acf4327d03",
   "metadata": {},
   "source": [
    "To measure the performance of the model with the class of interest, the predicted array will be transformed into a binary array where 1 indicates a predicted gun_shot class and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9de5c00-6851-44c3-9421-93619b5e5683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902/902 [==============================] - 1s 872us/step\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(test_ds) \n",
    "y_pred = np.argmax(predict,axis=1)\n",
    "y_test = [y.numpy()[0] for x, y in test_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1175508d-e968-4ddf-a0df-3912f07dccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_class = label_map['gun_shot']\n",
    "\n",
    "y_gun_shot_pred = [1 if x == interest_class else 0 for x in y_pred]\n",
    "y_gun_shot_test = [1 if x == interest_class else 0 for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4c5f613-debd-48c5-b215-6aec60cea600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       853\n",
      "           1       0.12      0.16      0.14        49\n",
      "\n",
      "    accuracy                           0.89       902\n",
      "   macro avg       0.54      0.55      0.54       902\n",
      "weighted avg       0.91      0.89      0.90       902\n",
      "\n",
      "Sensitivity:  0.16\n",
      "Specificity:  0.93\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(y_gun_shot_test, y_gun_shot_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b92d4-8f19-42c0-b578-08fb153eab0c",
   "metadata": {},
   "source": [
    "The previous report shows all relevant evaluation metrics for the model's performance with the class of interest. The metrics indicate that the model struggles to recognize gun_shot sounds. Although the model performs well in identifying noises that are not gun shots, it has a very low precision and recall for detecting gun shots, at only 12% precision and 16% recall.\n",
    "\n",
    "The confusion matrix analysis shows that the model has a high rate of labeling instances as \"other\" rather than correctly identifying them as gun shots. This suggests that the model is not well-suited for the current task of identifying gun shot sounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89dedd",
   "metadata": {},
   "source": [
    "Based on these results, it is absolutely necessary to consider other approaches to address the challenges of this approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78cc9dc8-09e0-4d4b-a65b-89c4e3f5b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a04c3dd-9e2f-4a22-877d-ec69de359bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('model_01.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df6157-3bf2-4b61-a385-ea073215fcb6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b02e7-72f0-493c-bd89-9a0dda9c201e",
   "metadata": {},
   "source": [
    "### Approach II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4189ced-dced-4984-9140-37ccd158e4a1",
   "metadata": {},
   "source": [
    "This approach follows the multi-feature stacking techniques discussed in [[1]](https://www.researchgate.net/publication/365912955_Multi-feature_stacking_order_impact_on_speech_emotion_recognition_performance). This method involves stacking multiple features extracted from the audio clip into a one-dimensional array. Five features will be computed from each audio: Mel-spectrogram, MFCCs, spectral contrast feature, chromagram and tonnetz. Each of these features will be then converted from a 2D matrix to a 1D array by taking the mean across the vertical axis. The authors in [[1]](https://www.researchgate.net/publication/365912955_Multi-feature_stacking_order_impact_on_speech_emotion_recognition_performance) stress the fact that the orders in which these features are stacked is significantly impactful on the final result. Therefore, following their recommended order, the features are horizontally stacked in the following order: spectral contrast, tonnetz, chromagram, Mel-spectrogram, and MFCC. The resulting vectors are then inputted into the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197f834-41eb-4c10-b11b-dfbec79e65c0",
   "metadata": {},
   "source": [
    "The satcked vectors are first loaded in preparation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33ccd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stacked_feature_arrays(split='train'):\n",
    "    path = './processed_data/stacked_features'\n",
    "    \n",
    "    print('loading {} set...'.format(split), end=' ')\n",
    "    X = np.loadtxt(\"{}/X_{}.csv\".format(path, split), delimiter=\",\")\n",
    "    y = np.loadtxt('{}/y_{}.csv'.format(path, split), delimiter = ',')\n",
    "    print('done!')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a335192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train set... done!\n",
      "loading val set... done!\n",
      "loading test set... done!\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_stacked_feature_arrays('train')\n",
    "X_val, y_val = load_stacked_feature_arrays('val')\n",
    "X_test, y_test = load_stacked_feature_arrays('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682f1d2-156e-43aa-82d8-322e5fd6e60d",
   "metadata": {},
   "source": [
    "---\n",
    "Two different classification models are trained for comparison, Support vector machine (SVM) and a 1D convolutional neural network (CNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab88f1-3855-4b15-b253-c02559627e6d",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da11035d-05f3-484f-a655-dcb57800acfd",
   "metadata": {},
   "source": [
    "To find the best performing set of parameters, a grid search will be performed with scikit-learn's SVC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f152eaaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.163 total time=   3.8s\n",
      "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.163 total time=   3.8s\n",
      "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.163 total time=   3.8s\n",
      "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.163 total time=   3.8s\n",
      "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.163 total time=   3.8s\n",
      "[CV 1/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.123 total time= 2.7min\n",
      "[CV 2/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.136 total time= 2.3min\n",
      "[CV 3/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.121 total time= 2.6min\n",
      "[CV 4/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.110 total time= 2.6min\n",
      "[CV 5/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.102 total time= 2.4min\n",
      "[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.163 total time=   3.3s\n",
      "[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.163 total time=   3.2s\n",
      "[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.163 total time=   3.3s\n",
      "[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.163 total time=   3.3s\n",
      "[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.163 total time=   3.3s\n",
      "[CV 1/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.123 total time= 2.6min\n",
      "[CV 2/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.136 total time= 2.3min\n",
      "[CV 3/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.121 total time= 2.6min\n",
      "[CV 4/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.110 total time= 2.6min\n",
      "[CV 5/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.102 total time= 2.5min\n",
      "[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.162 total time=   2.7s\n",
      "[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.167 total time=   2.7s\n",
      "[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.177 total time=   2.7s\n",
      "[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.170 total time=   2.6s\n",
      "[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.163 total time=   2.6s\n",
      "[CV 1/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.123 total time= 2.6min\n",
      "[CV 2/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.136 total time= 2.3min\n",
      "[CV 3/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.121 total time= 2.5min\n",
      "[CV 4/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.110 total time= 2.6min\n",
      "[CV 5/5] END .C=0.1, gamma=0.001, kernel=linear;, score=0.102 total time= 2.4min\n",
      "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.194 total time=   3.8s\n",
      "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.166 total time=   3.9s\n",
      "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.164 total time=   3.9s\n",
      "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.189 total time=   4.1s\n",
      "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.203 total time=   4.1s\n",
      "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.136 total time=30.7min\n",
      "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.121 total time=23.2min\n",
      "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.120 total time=26.8min\n",
      "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.114 total time=28.7min\n",
      "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.104 total time=24.8min\n",
      "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.224 total time=   3.4s\n",
      "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.165 total time=   3.3s\n",
      "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.183 total time=   3.3s\n",
      "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.187 total time=   3.3s\n",
      "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.205 total time=   3.3s\n",
      "[CV 1/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.136 total time=30.5min\n",
      "[CV 2/5] END ...C=1, gamma=0.01, kernel=linear;, score=0.121 total time=486.0min\n",
      "[CV 3/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.120 total time=79.2min\n",
      "[CV 4/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.114 total time=29.6min\n",
      "[CV 5/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.104 total time=25.4min\n",
      "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.199 total time=   2.9s\n",
      "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.173 total time=   2.8s\n",
      "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.171 total time=   2.8s\n",
      "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.159 total time=   2.7s\n",
      "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.192 total time=   2.8s\n",
      "[CV 1/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.136 total time=31.1min\n",
      "[CV 2/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.121 total time=23.5min\n",
      "[CV 3/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.120 total time=27.1min\n",
      "[CV 4/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.114 total time=29.5min\n",
      "[CV 5/5] END ...C=1, gamma=0.001, kernel=linear;, score=0.104 total time=25.5min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, gamma=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=1, gamma=0.01)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, gamma=0.01)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm_estimator = SVC()\n",
    "svm_parameters = {'C': [0.1,1], 'gamma': [0.1,0.01,0.001], 'kernel': ['rbf', 'linear']}\n",
    "svm = GridSearchCV(svm_estimator, svm_parameters, verbose=3)\n",
    "svm.fit(X_train, y_train)\n",
    "svm.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c34e4e7-4a2e-4733-a5be-6ce72d9f7a39",
   "metadata": {},
   "source": [
    "To validate the resulting model performance, the model is tested against the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f588d063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model validation accuracy: 0.18\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_val_pred = svm.predict(X_val)\n",
    "\n",
    "print('Model validation accuracy: {}'.format(round(accuracy_score(y_val, y_val_pred), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "945d63c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save the best estimator to a file\n",
    "dump(svm.best_estimator_, 'svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "003d15f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming your SVM model is trained\n",
    "y_train_pred = svm.predict(X_train)\n",
    "y_val_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "val_accuracy = accuracy_score(y_test, y_val_pred)\n",
    "\n",
    "# Prepare data for plotting\n",
    "datasets = ['Training', 'Validation']\n",
    "accuracies = [train_accuracy, val_accuracy]\n",
    "\n",
    "# Plot the chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(datasets, accuracies, color=['skyblue', 'orange'])\n",
    "plt.ylim(0, 1)  # Accuracy ranges between 0 and 1\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Dataset')\n",
    "plt.text(0, train_accuracy + 0.02, f'{train_accuracy:.2f}', ha='center', fontsize=12)\n",
    "plt.text(1, val_accuracy + 0.02, f'{val_accuracy:.2f}', ha='center', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3513596-57a0-4cd0-9358-2b77922c2356",
   "metadata": {},
   "source": [
    "The model scored a validation accuracy of 75% which is already very promising for the current method. For baseline comparison and further testing the validity of the feature stacking approach, a 1D CNN will be also trained and validated on the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e4c3c-f072-456d-b814-8ab240def58b",
   "metadata": {},
   "source": [
    "### 1D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34709060-3eaa-4c60-80ae-1ede7c4ca937",
   "metadata": {},
   "source": [
    "The following CNN structure will be used. The CNN takes a one dimenstional input vector equal to the length of the stacked vectors. The input first goes throgh 2 consecutive 1D convolutional layer with ReLU activation function. The output is then randomly dropped to 50% to avoid overfitting before it undergoes pooling. The pooling output is passed to a two consecutive dense layers, the first contains 128 nodes and the last one consists of 10 nodes representing the 10 classes. The CNN is compiled using the adam optimiser and the accuracy will be used as an evaluation metric for fine-tunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cdfe59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_features, n_outputs = X_train.shape[0], X_train.shape[1], y_train.shape[0]\n",
    "input_shape = (n_features, 1)\n",
    "num_classes = 10\n",
    "\n",
    "model_1D = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(32, 3, activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.Conv1D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.MaxPooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daa11b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1D.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a13a2db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 2. 1. ... 9. 9. 9.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6adadbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.20523672 10.8770794  16.24185323 ...  3.17177129  0.81301951\n",
      "   2.17169809]\n",
      " [14.82025378 11.91080723 16.52717196 ... -1.94387448 -2.55152774\n",
      "   1.13308632]\n",
      " [14.13180072 10.82789806 13.72830239 ...  3.47977805  0.51236737\n",
      "   2.8376739 ]\n",
      " ...\n",
      " [12.5061171  11.18990634 18.34669845 ...  5.37196541  3.30374312\n",
      "  -1.8794322 ]\n",
      " [24.36818788 14.05271472 14.77137906 ...  0.45892271 -2.58513594\n",
      "  -0.54989058]\n",
      " [13.85791636 10.85135255 13.94136371 ...  3.21099567  0.26116559\n",
      "   2.74788666]]\n",
      "[[[11.20523672]\n",
      "  [10.8770794 ]\n",
      "  [16.24185323]\n",
      "  ...\n",
      "  [ 3.17177129]\n",
      "  [ 0.81301951]\n",
      "  [ 2.17169809]]\n",
      "\n",
      " [[14.82025378]\n",
      "  [11.91080723]\n",
      "  [16.52717196]\n",
      "  ...\n",
      "  [-1.94387448]\n",
      "  [-2.55152774]\n",
      "  [ 1.13308632]]\n",
      "\n",
      " [[14.13180072]\n",
      "  [10.82789806]\n",
      "  [13.72830239]\n",
      "  ...\n",
      "  [ 3.47977805]\n",
      "  [ 0.51236737]\n",
      "  [ 2.8376739 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[12.5061171 ]\n",
      "  [11.18990634]\n",
      "  [18.34669845]\n",
      "  ...\n",
      "  [ 5.37196541]\n",
      "  [ 3.30374312]\n",
      "  [-1.8794322 ]]\n",
      "\n",
      " [[24.36818788]\n",
      "  [14.05271472]\n",
      "  [14.77137906]\n",
      "  ...\n",
      "  [ 0.45892271]\n",
      "  [-2.58513594]\n",
      "  [-0.54989058]]\n",
      "\n",
      " [[13.85791636]\n",
      "  [10.85135255]\n",
      "  [13.94136371]\n",
      "  ...\n",
      "  [ 3.21099567]\n",
      "  [ 0.26116559]\n",
      "  [ 2.74788666]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming X_train is a NumPy array\n",
    "X_train_reshaped = X_train.reshape(-1, 193, 1)\n",
    "\n",
    "print(X_train)\n",
    "print(X_train_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f40e44-2fb4-4911-a480-cc8df1eb7c48",
   "metadata": {},
   "source": [
    "The model is trained in 20 epochs and the validation sets will be used for model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c592f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1D.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),  # Change here\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc67f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Repeat the labels for each timestep\n",
    "y_train_adjusted2 = np.tile(y_train, (94,1)).T  # Adjust 94 based on sequence length\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train_adjusted2, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c39e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1D.build((None, 193, 1))  # Adjust 193 and 1 to match your input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e74a330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5691, 193)\n",
      "y_train shape: (5691,)\n",
      "X_val shape: (1475, 193)\n",
      "y_val shape: (1475,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape if X_train is not None else None}\")\n",
    "print(f\"y_train shape: {y_train.shape if y_train is not None else None}\")\n",
    "print(f\"X_val shape: {X_val.shape if X_val is not None else None}\")\n",
    "print(f\"y_val shape: {y_val.shape if y_val is not None else None}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "74137583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train reshaped: (5691, 193, 1)\n",
      "X_val reshaped: (1475, 193, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape input data to add channel dimension\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_val_reshaped = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "\n",
    "print(f\"X_train reshaped: {X_train_reshaped.shape}\")  # Should be (5691, 193, 1)\n",
    "print(f\"X_val reshaped: {X_val_reshaped.shape}\")      # Should be (1475, 193, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "41f113ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_one_hot shape: (5691, 10)\n",
      "y_val_one_hot shape: (1475, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10  # Replace with the actual number of classes in your dataset\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val_one_hot = to_categorical(y_val, num_classes=num_classes)\n",
    "\n",
    "print(f\"y_train_one_hot shape: {y_train_one_hot.shape}\")  # Should be (5691, 10)\n",
    "print(f\"y_val_one_hot shape: {y_val_one_hot.shape}\")      # Should be (1475, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1e19356c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">191</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> \n",
       "\n",
       " conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">189</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> \n",
       "\n",
       " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">189</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> \n",
       "\n",
       " dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m191\u001b[0m, \u001b[38;5;34m32\u001b[0m)                   \u001b[38;5;34m128\u001b[0m \n",
       "\n",
       " conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m189\u001b[0m, \u001b[38;5;34m32\u001b[0m)                 \u001b[38;5;34m3,104\u001b[0m \n",
       "\n",
       " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m189\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_4 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m2,112\u001b[0m \n",
       "\n",
       " dense_5 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m10\u001b[0m)                    \u001b[38;5;34m650\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,984</span> (70.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,984\u001b[0m (70.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,994</span> (23.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,994\u001b[0m (23.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,990</span> (46.84 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m11,990\u001b[0m (46.84 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_1D.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4e896548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_reshaped: (5691, 193, 1)\n",
      "X_val_reshaped: (1475, 193, 1)\n",
      "y_train_one_hot: (5691, 10)\n",
      "y_val_one_hot: (1475, 10)\n"
     ]
    }
   ],
   "source": [
    "# Check data and label shapes\n",
    "print(f\"X_train_reshaped: {X_train_reshaped.shape if X_train_reshaped is not None else 'None'}\")\n",
    "print(f\"X_val_reshaped: {X_val_reshaped.shape if X_val_reshaped is not None else 'None'}\")\n",
    "print(f\"y_train_one_hot: {y_train_one_hot.shape if y_train_one_hot is not None else 'None'}\")\n",
    "print(f\"y_val_one_hot: {y_val_one_hot.shape if y_val_one_hot is not None else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba6804f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data shape: (1475, 193, 1), (1475, 10)\n"
     ]
    }
   ],
   "source": [
    "# Check validation data\n",
    "validation_data = (X_val_reshaped, y_val_one_hot)\n",
    "print(f\"Validation data shape: {validation_data[0].shape}, {validation_data[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "99907357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(1, 10), output.shape=(1, 94, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_1D\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_one_hot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_one_hot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:580\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same rank \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(ndim). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    584\u001b[0m     )\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(1, 10), output.shape=(1, 94, 10)"
     ]
    }
   ],
   "source": [
    "model_1D.fit(\n",
    "    X_train_reshaped,\n",
    "    y_train_one_hot,\n",
    "    validation_data=(X_val_reshaped, y_val_one_hot),\n",
    "    epochs=20,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807b487",
   "metadata": {},
   "source": [
    "While training the model, the highest validation accuracy achieved was 86.33% reached on the 16th epoch with a loss of 0.44. Afterwards, the validation accuracy did not show significant change. The average accuracy of the last 5 epochs was 83.72%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec7757",
   "metadata": {},
   "source": [
    "**This is a higher accuracy than the value achieved by the SVC. This shows a great promise for the multi-feature stacking approach with deep learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298e576-ba86-434d-b30a-ebf8d8ec633e",
   "metadata": {},
   "source": [
    "## Testing for approach II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ede76-5ed9-4af1-a713-a5dc1696c9e4",
   "metadata": {},
   "source": [
    "Since the CNN model outperformed the SVC model in the validation phase, the CNN is chosen as the better model for the current approach. To further test the suitability of the CNN model for the task at hand, the model is tested against the **test set split**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83e86511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 1ms/step\n",
      "Model validation accuracy: 80.04%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model_1D.predict(X_test)\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "print('Model validation accuracy: {}%'.format(round(accuracy_score(y_test, y_pred) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3908d0",
   "metadata": {},
   "source": [
    "The general accuracy score for the model is 80.04%, however, this does not represent the real accuracy of the model for the gunshot detection task. To measure the performance of the model with the class of interest, the predicted array will be transformed into a binary array where 1 indicates a predicted gun_shot class and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bac30f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_class = label_map['gun_shot']\n",
    "\n",
    "y_gun_shot_pred = [1 if x == interest_class else 0 for x in y_pred]\n",
    "y_gun_shot_test = [1 if x == interest_class else 0 for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fca2d584",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       853\n",
      "           1       0.98      0.86      0.91        49\n",
      "\n",
      "    accuracy                           0.99       902\n",
      "   macro avg       0.98      0.93      0.95       902\n",
      "weighted avg       0.99      0.99      0.99       902\n",
      "\n",
      "Sensitivity:  0.86\n",
      "Specificity:  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "report(y_gun_shot_test, y_gun_shot_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46585ff3",
   "metadata": {},
   "source": [
    "Based on the evaluation metrics presented in the previous report, the CNN model for gunshot detection achieved excellent results. The model was able to discriminate between the gun_shot class and all other classes with nearly perfect scores, resulting in an overall accuracy of 99%. The model also showed strong sensitivity (or recall) with a score of 86%, indicating that the model can accurately detect most instances of gun_shot sounds. In addition, the model demonstrated a perfect specificity, which means that it mostly did not misclassify any non-gun_shot sounds as gun_shot sounds. These results suggest that the CNN model is highly effective at detecting gun_shot sounds, and could be a valuable tool in identifying and responding to potential gun-related incidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04ec78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1D.save('model_02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b34d0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1D = tf.keras.models.load_model('model_02.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94ea99",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a88cbca",
   "metadata": {},
   "source": [
    "Although the model is already tested on the test split, which was unseen by the model before, it is also necessary to evaluate the model performance with data obtained outside of the current dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afefaf",
   "metadata": {},
   "source": [
    "In this step, the trained model is applied to fresh, unseen data to evaluate its performance in real-world scenarios. This phase involves testing the model on a sample of data that was not previously used in the training, validation or testing phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18e556",
   "metadata": {},
   "source": [
    "In the current project, the model is tested on 10 different sound clips downloaded from YouTube in mp3 format. This is a small sample due to the limited time available for the project, but it serves as an experiment to assess how the model would perform with real-world data. The 10 sound clips belong to different classes, but not all classes are represented in the sample. Some of the sound clips contain gunshot sounds representing the target class. The performance of the model on these sound clips can help evaluate its efficacy in detecting gunshot sounds in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c938eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCING_DIR = './inferencing_set'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8722269",
   "metadata": {},
   "source": [
    "The files are loaded into a numpy array to prepare for inferencing. The sounds are loaded as 2-second-segments. If a sound is smaller, it is padded with silence. If the sound is longer, the first 2 seconds are taken. The original sound files all have a lengths of few ms seconds less or more than 2 seconds; thus, the lengths are not greatly affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a2e07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 44100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = librosa.util.find_files(INFERENCING_DIR, ext=['mp3'])\n",
    "files = np.asarray(files)\n",
    "LEN = 44100\n",
    "\n",
    "inf_slices = []\n",
    "inf_labels = []\n",
    "\n",
    "for file in files:\n",
    "    if '.mp3' in file:\n",
    "        filename = file.split('/').pop()\n",
    "        label = filename.split('.')[0].split('-')[1] # class ID\n",
    "        wave_arr, sr = librosa.load(file, sr = SAMPLE_RATE, mono = True)\n",
    "        \n",
    "        if len(wave_arr) < 44100:\n",
    "            diff = LEN - len(wave_arr)\n",
    "            silence = np.zeros(diff) # silence\n",
    "            lead = silence[0 : math.ceil(diff / 2)]\n",
    "            trail = silence[0 : math.floor(diff / 2)]\n",
    "            \n",
    "            wave_arr = np.concatenate((lead, wave_arr, trail))\n",
    "        \n",
    "        inf_slices += [wave_arr.astype(float)[:LEN]]\n",
    "        inf_labels += [int(label)]\n",
    "        \n",
    "inf_slices = np.array(inf_slices, dtype=np.float)\n",
    "y_inf = np.array(inf_labels)\n",
    "\n",
    "inf_slices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a7880",
   "metadata": {},
   "source": [
    "The next step is to extract the stacked features vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339570cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing set stacked features: extracting........done!\n"
     ]
    }
   ],
   "source": [
    "print('Inferencing set stacked features: ', end='')\n",
    "X_stack_inf = extract_stacked_feature_arrays(inf_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca30965",
   "metadata": {},
   "source": [
    "Next, the model is loaded then the sounds arrays passed to the model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2587c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 01:29:43.042701: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-04-14 01:29:43.042764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: CMPR01\n",
      "2023-04-14 01:29:43.042781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: CMPR01\n",
      "2023-04-14 01:29:43.043002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.182.3\n",
      "2023-04-14 01:29:43.043061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.161.3\n",
      "2023-04-14 01:29:43.043076: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 470.161.3 does not match DSO version 470.182.3 -- cannot find working devices in this configuration\n",
      "2023-04-14 01:29:43.044171: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model_02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac1a099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_stack_inf)\n",
    "y_pred = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ddb28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_class = label_map['gun_shot']\n",
    "\n",
    "y_gun_shot_pred = [1 if x == interest_class else 0 for x in y_pred]\n",
    "y_gun_shot_test = [1 if x == interest_class else 0 for x in y_inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c69efda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         6\n",
      "           1       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.88      0.75      0.76        10\n",
      "weighted avg       0.85      0.80      0.78        10\n",
      "\n",
      "Sensitivity:  0.5\n",
      "Specificity:  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "report(y_gun_shot_test, y_gun_shot_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f15a3",
   "metadata": {},
   "source": [
    "The previous report shows all evaluation metrics for the quality of the inferencing of the model. It is clear that the model achieved an excellent accuracy of 80%, which is a promising outcome. However, this value is lower than the accuracy score obtained on the test set, which was 99%. This variation is not unusual since the test set consists of carefully curated data that the model had never encountered before but they are extracted from the same sources. Whereas the fresh data in the inference phase is pulled from the internet and may contain variables that the model has not been trained on.\n",
    "\n",
    "The report also shows that the model was successful in predicting the target class with 100% precision. This means that of all the instances predicted as the target class, every single one was correct. However, the recall score for the target class was only 50%. This means that the model correctly identified only half of the actual target class instances in the sample. This could indicate that the model is not as sensitive to detecting the target class as it should be, and may require further fine-tuning or adjustment to improve its performance in this area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc609ec7",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fe45eb",
   "metadata": {},
   "source": [
    "The 1D multi-feature stacking technique proves to be superior to the image mel-spectrogram classification in detecting noise in the given task. The accuracy achieved by the SVC classifier and 1D CNN surpassed that of the 2D CNN in Approach I. Furthermore, when assessing the model's ability to identify the class of interest as opposed to all other classes, the superiority of the 1D multi-feature stacking technique becomes even more apparent. Combining all other classes into a single class showed that the first approach struggled to identify the majority of instances of the target class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa22c0",
   "metadata": {},
   "source": [
    "The poor performance of the first approach could be due to many reasons including the nature of the gun_shot class. Gunshots are loud and quick. Other types of noises, such as children_playing, happen continously over an extended period of time and they have a relevantly smooth beginning and end. On the other hand, gunshots occur suddenly and their sounds go from extremely low frequency (zero if no background noise is present) to a momentarily very high frequency then back to low frequency again. This results in mel-spectrograms similar to the two examples displayed below. As seen below, the images are mostly black with a small portion of sound variation. Similar images might be confusing for the CNN as they don't provide a big variation in features and they are mostly blank. This might caused the CNN to struggle in learning the right features for the interest class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15350932",
   "metadata": {},
   "source": [
    "<img src=\"data:image/jpeg;base64,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\" \n",
    "/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d39aa8",
   "metadata": {},
   "source": [
    "Furthermore, computer vision tasks require a huge amount of data in order to achieve satisfactory results. The training set included 700 instances per class which is relatively small for similar tasks. More instances and more variation of sound environments could benifit the model for a better detection accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6516e1e",
   "metadata": {},
   "source": [
    "The multi-feature stacking solves those issues by providing much more data about each sound. This approach provides a more comprehensive understanding of each sound, enhancing the model's ability to classify them accurately. By combining the knowledge gained from mel-spectrogram representations with other audio features, the model gains greater insight into the nature of the sound and its various aspects, resulting in improved classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee6fd1",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baf408",
   "metadata": {},
   "source": [
    "This notebook details the implementation of a gunshot detection system using deep learning. A dataset of 8732 labelled sound files divided into 10 different classes was used. The necessary data preprocessing techniques are applied to the data to prepare for training and to address some issues with the data such as the class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68445aa8",
   "metadata": {},
   "source": [
    "Two approaches were tested for the task, with the first one involving the conversion of sound files into mel-spectrogram images. A 2D CNN model was then trained and evaluated on these images for mel-spectrogram image classification. Although the model achieved a 73% accuracy in classifying sounds into any of the ten classes, it performed poorly in identifying the target class, i.e., gunshot sounds. Therefore, this approach was found unsuitable for the current task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff21a1",
   "metadata": {},
   "source": [
    "The second approach for the gunshot detection system involves extracting five distinct 2D features from each sound instance. These features are then transformed into 1D vectors by averaging across the x-axis, and subsequently stacked in a particular order to form a long 1D vector representing the sound instance. Two models were trained using this approach, SVC and 1D CNN. During the validation phase, both models demonstrated impressive results, with the CNN outperforming the SVC with validation accuracy scores of 86% and 75%, respectively. The CNN model was then evaluated on the test set, and it produced remarkable outcomes.\n",
    "\n",
    "The CNN model achieved an overall classification accuracy of 80% in identifying sounds among the ten classes. However, when examined for the target class, the model demonstrated a 99% accuracy in distinguishing gunshot sounds from all other classes. This result confirms that this approach utilizing deep learning is the most appropriate for the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104352ae",
   "metadata": {},
   "source": [
    "When applied to fresh unseen sound clips pulled from the internet, the 1D CNN achieved an accuracy of 80%. While this value is lower than the accuracy score obtained on the test set, it is still a satisfactory result. The precision score for the target class was 100%, however, the recall score for the target class was only 50%, suggesting that the model may require further improvement to increase its sensitivity to detecting the target class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751bc57",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb74993",
   "metadata": {},
   "source": [
    "\\[1\\]   Y. Tanoko and A. Zahra, Multi-feature stacking order impact on speech emotion recognition performance, Bulletin of Electrical Engineering and Informatics, vol. 11, no. 6, pp. 32723278, Dec. 2022, doi: 10.11591/EEI.V11I6.4287."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2f0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
