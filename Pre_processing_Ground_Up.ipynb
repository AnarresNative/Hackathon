{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to start a ground  u p rework fothe data based on features suggested by our favourite co-pilot. It seems like we can get a decent answer based on a lightweight model, by processing the audio files using librosa. The features we're looking to extract are: Mel-Frequency Spectral Coefficients(MFCC) \n",
    "Spectral Centroid\n",
    "Spectral Bandwidth\n",
    "Spectral Flatness\n",
    "Spectral Contrast\n",
    "Time Domain features - Zero Crossing rate (ZCR)\n",
    "Root Mean Square Energy (RMSE)\n",
    "Temporal features - Short-time energy\n",
    "Tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips for Efficient Processing on Raspberry Pi 5\n",
    "\n",
    "    Downsampling:\n",
    "        Use a lower sampling rate (e.g., 16 kHz) unless high-frequency information is crucial.\n",
    "    Windowing:\n",
    "        Apply short-time Fourier transform (STFT) with small windows (e.g., 20â€“50 ms) for manageable computational loads.\n",
    "    Batch Processing:\n",
    "        Process audio in chunks to avoid memory and CPU spikes.\n",
    "    Feature Dimensionality Reduction:\n",
    "        Reduce the number of features using techniques like Principal Component Analysis (PCA) after extraction.\n",
    "    Quantized Models:\n",
    "        Use a lightweight, quantized ML model optimized for edge devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import basic pacages for data processing . We're going to use Librosa to process the data as it seems to be  more lightweight than alternatives.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "sample_rate = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of audio, we are resampling, normalising and trimming silence.\n",
    "\n",
    "Resampling will normalise the sampling rate, reducing one variable between datasets. I assume this means that we will want to resamle audio in the input stream.\n",
    "\n",
    "Normalise standardises the audio amplitude, apparently mimising the influence of recording volume, to prevent outlier high or low sounds from affecting the  model training. \n",
    "\n",
    "Trimming silence to remove useless pieces of data to save on computation, and reduce the affect silence could have on affecting our model weights. (Need to check this, feel like it should be heavily caveated.) I have now checked this, it makes sense that silence being a recurring feature across classes could lead to issues, but having a separate class specifically for silence callsed (no event) would be good potentially.\n",
    "\n",
    "Below is initial set  up of the functions and importing of metadata for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def initialise_array(x):\n",
    "    # Specify the folder path\n",
    "    folder_path = f'./Audio/fold{x}'\n",
    "\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Filter out non-file entries (e.g., directories), and select only .wav files\n",
    "    files = [f for f in files if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.wav')]\n",
    "\n",
    "    # Count the number of .wav files\n",
    "    num_files = len(files)\n",
    "\n",
    "    # Initialize a blank NumPy array of the same length\n",
    "    # You can initialize with NaN or None depending on your needs\n",
    "    blank_array = np.zeros(num_files, dtype=object)  # Or np.nan, or np.zeros(num_files) depending on your use case\n",
    "    blank_array[:] = np.nan # placeholder array for audio slices\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Number of .wav files: {num_files}\")\n",
    "    print(\"Initialized NumPy array:\", np.size(blank_array))\n",
    "    return(blank_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .wav files: 873\n",
      "Initialized NumPy array: 873\n"
     ]
    }
   ],
   "source": [
    "AUDIO_DIR = './Audio' # path to audio files directory\n",
    "x = 1\n",
    "folders = ['fold{}'.format(x)] # array of paths to each audio folder\n",
    "slices = initialise_array(x=x)\n",
    "\n",
    "# Load the metadata:\n",
    "metadata = pd.read_csv('./data-description.csv')\n",
    "metadata['length'] = metadata['end'] - metadata['start']\n",
    "\n",
    "# Label map of the different sound classes:\n",
    "label_map = {\n",
    "    'air_conditioner': 0,\n",
    "    'car_horn': 1,\n",
    "    'children_playing': 2,\n",
    "    'dog_bark': 3,\n",
    "    'drilling': 4,\n",
    "    'engine_idling': 5,\n",
    "    'gun_shot': 6,\n",
    "    'jackhammer': 7,\n",
    "    'siren': 8,\n",
    "    'street_music': 9,\n",
    "}\n",
    "\n",
    "def get_class_name(idx):\n",
    "    return list(label_map.keys())[list(label_map.values()).index(idx)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(metadata['class']) # list of audio classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting fold1..."
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 877 is out of bounds for axis 0 with size 873",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m         wave_arr, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(file, sr \u001b[38;5;241m=\u001b[39m sample_rate, mono \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m         idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(metadata\u001b[38;5;241m.\u001b[39mindex[metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslice_file_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m name])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m         \u001b[43mslices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m wave_arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 877 is out of bounds for axis 0 with size 873"
     ]
    }
   ],
   "source": [
    "for fold in folders:\n",
    "    print('collecting {}...'.format(fold), end = \"\")\n",
    "    files = librosa.util.find_files('{}/{}'.format(AUDIO_DIR, fold), ext=['wav'])\n",
    "    files = np.asarray(files)\n",
    "    for file in files:\n",
    "        if '.wav' in file:\n",
    "            name = file.split('/').pop()\n",
    "            wave_arr, sr = librosa.load(file, sr = sample_rate, mono = True)\n",
    "            idx = list(metadata.index[metadata['slice_file_name'] == name])[0]\n",
    "            slices[idx] = wave_arr.astype(object)\n",
    "    print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan ... nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "print(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Mismatch between array dtype ('object') and format specifier ('%.18e')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1626\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1626\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m newline\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavetxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray_no_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1628\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1626\u001b[0m             v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtuple\u001b[39m(row) \u001b[38;5;241m+\u001b[39m newline\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1628\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch between array dtype (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1629\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat specifier (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1630\u001b[0m                             \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mstr\u001b[39m(X\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28mformat\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m         fh\u001b[38;5;241m.\u001b[39mwrite(v)\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(footer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Mismatch between array dtype ('object') and format specifier ('%.18e')"
     ]
    }
   ],
   "source": [
    "np.savetxt(\"output.csv\", array_no_nan, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_no_nan = np.nan_to_num(slices, nan=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, it is advisable to split the formatted audio files into several **equal** brief segments with predetermined time intervals. For this project, a threshold value of 2 seconds is set for all clips. Two seconds is a suitable threshold as it is equal to half of the maximum lengths of all classes. It is also slightly higher than the mean length of the interest class (1.65 s) and less than the mean length across all classes (3.61 s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_len = 2\n",
    "length_of_wave_arr = round(goal_len * sample_rate) # the length of the desired slices segments arrays\n",
    "length_of_wave_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to issues with RAM, I'm only loading a small amount ofthe 8700 files at a time, leaving nan values in teh initialised array, this will break the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_slices(slices, classes, split_length=length_of_wave_arr):\n",
    "    X = slices.copy()\n",
    "    y = classes.copy()\n",
    "    \n",
    "    idx = 0\n",
    "    while idx < X.shape[0]:\n",
    "        _slice = X[idx]\n",
    "        _class = y[idx]\n",
    "        \n",
    "        if(_slice.shape[0] == split_length): # If it is already 2 seconds long, skip.\n",
    "            idx += 1\n",
    "            continue\n",
    "            \n",
    "        elif(_slice.shape[0] < split_length):  # If it is less than 2 seconds long\n",
    "            diff = split_length - _slice.shape[0]\n",
    "            silence = np.zeros(split_length) # silence\n",
    "            lead = silence[0 : math.ceil(diff / 2)]\n",
    "            trail = silence[0 : math.floor(diff / 2)]\n",
    "            \n",
    "            X[idx] = np.concatenate((lead, _slice, trail)) # pad with silence\n",
    "        \n",
    "        else:\n",
    "            # split into two segments\n",
    "            seg1 = _slice[:length_of_wave_arr]\n",
    "            seg2 = _slice[length_of_wave_arr:]\n",
    "            \n",
    "            X[idx] = seg1\n",
    "            \n",
    "            # If it is longer than 2 s and belongs to the small classes\n",
    "            # add to the queue to undergo another split/silence padding\n",
    "            if(seg2.shape[0] >= MIN_LEN and (_class == 'gun_shot' or _class == 'car_horn')):\n",
    "                X = np.array(list(X) + list(np.array([seg2])), dtype=object )\n",
    "                y = np.append(y, _class)\n",
    "                \n",
    "        idx += 1\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(slices.copy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43msplit_audio_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m, in \u001b[0;36msplit_audio_slices\u001b[0;34m(slices, classes, split_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m _slice \u001b[38;5;241m=\u001b[39m X[idx]\n\u001b[1;32m      8\u001b[0m _class \u001b[38;5;241m=\u001b[39m y[idx]\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[43m_slice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m split_length): \u001b[38;5;66;03m# If it is already 2 seconds long, skip.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "X, y = split_audio_slices(slices, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to start with a single wav file as a test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestAudioPath = \"audio/fold1/7061-6-0-0.wav\"\n",
    "\n",
    "def extract_features(file_path, sr=16000, n_mfcc=13):\n",
    "    # Load the audio file\n",
    "    audio, sample_rate = librosa.load(TestAudioPath, sr=sr)\n",
    "\n",
    "    # Extract the MFCC features\n",
    "    mfccs = librosa.features.mfcc(y=audio, sr=sample_rate, n_mfcc = n_mfcc)\n",
    "\n",
    "    # Compute the mean of the extracted features across  frames\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've questioned chatgpt on why it wants  to normalise the MFCCs to a human frequency (using the mel scale.) It has agreed this doesnt really make sense. I think I will try to go down the path of doing both mel scale and none mel scale feature sets, build a model from both and see how they compare. Potentially may be worth stacking them? I don't see how applying a log scale is going to help much though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spectral_features(audio, sr = 16000):\n",
    "    \n",
    "    audio, sample_rate = librosa.load(audio, sr=sr)\n",
    "    print(\"Audio data type:\", type(audio))\n",
    "    #  Short-Time Fourier Transform (STFT)\n",
    "    # Audio is the inputted audio waveform, it transforms it into the time frequency domain.\n",
    "    # we use abs becase it takes the magnitude, or the \"strength\" of each frequency\n",
    "    # np.abs is a numpy feature to take an array and convert all values to positives.\n",
    "    stft = np.abs(librosa.stft(audio))\n",
    "\n",
    "    \n",
    "    # Spectral Features, the spectral centroied is seen as the \"brightness\" of a sound/ Gunshots typically? Have higher spectral centroids due to high energy  at high frequencies.\n",
    "    centroid = np.mean(librosa.feature.spectral_centroid(S=stft, sr=sr))\n",
    "    # the bandwidth shows the width of the distributionn of frequencies, according to ChatGPT gunshots will have a higher than typical sound badnnwidth\n",
    "    bandwidth = np.mean(librosa.feature.spectral_bandwidth(S=stft, sr=sr))\n",
    "    # Apparently Gunshots are also generally high spectral flatness.\n",
    "    flatness = np.mean(librosa.feature.spectral_flatness(S=stft))\n",
    "    \n",
    "    # Temporal Features. The onsert_env seems like a really important one, it's the sharpness/prominence of the onset of the sound.\n",
    "    # Due to what a gunshot is, I expect this to be MASSIVe compared to a usual sound.\n",
    "    onset_env = librosa.onset.onset_strength(y=audio, sr=sr)\n",
    "    # Captures the maximum intensity of the sound, which will probably be higher than an average sound.\n",
    "    peak_amplitude = np.max(np.abs(audio))\n",
    "    \n",
    "    # Combine features into a single array representing the audio clip\n",
    "    features = [centroid, bandwidth, flatness, onset_env, peak_amplitude]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to try the feature set as described above. But want to bare in mind Mariams paper. The authors in [1] stress the fact that the orders in which these features are stacked is significantly impactful on the final result. Therefore, following their recommended order, the features are horizontally stacked in the following order: spectral contrast, tonnetz, chromagram, Mel-spectrogram, and MFCC. The resulting vectors are then inputted into the classifier.\n",
    "\n",
    "Will try and test both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12691/3903765992.py:3: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sample_rate = librosa.load(audio, sr=sr)\n",
      "/home/anarres-native/Hackathon/venv/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'audio/fold1/7061-6-0-0.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/librosa/core/audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening 'audio/fold1/7061-6-0-0.wav': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m TestAudioPath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio/fold1/7061-6-0-0.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# For each audio clip, append its features\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_spectral_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTestAudioPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m all_features\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create a DataFrame\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mextract_spectral_features\u001b[0;34m(audio, sr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_spectral_features\u001b[39m(audio, sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16000\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     audio, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio data type:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(audio))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#  Short-Time Fourier Transform (STFT)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Audio is the inputted audio waveform, it transforms it into the time frequency domain.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# we use abs becase it takes the magnitude, or the \"strength\" of each frequency\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# np.abs is a numpy feature to take an array and convert all values to positives.\u001b[39;00m\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/librosa/core/audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/librosa/util/decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/librosa/core/audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/audioread/__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Hackathon/venv/lib/python3.12/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'audio/fold1/7061-6-0-0.wav'"
     ]
    }
   ],
   "source": [
    "all_features = []  # A list to hold features for each audio clip\n",
    "\n",
    "#Single test audio file for now, need to  do some processing of files first.\n",
    "TestAudioPath = \"audio/fold1/7061-6-0-0.wav\"\n",
    "\n",
    "# For each audio clip, append its features\n",
    "features = extract_spectral_features(TestAudioPath, sr=16000)\n",
    "all_features.append(features)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(all_features, columns=[\"Centroid\", \"Bandwidth\", \"Flatness\",\"Onset Env\", \"PeakAmplitude\"])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"spectral_features.csv\", index=False)\n",
    "\n",
    "# Check the extracted features\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to try and run a sample of different sounds and see what the different types of sound look like visually. First need to do some data loading of the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata:\n",
    "metadata = pd.read_csv('./data-description.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the dataframe named `metadata' represents the metadata for one audio file. For each audio file, the following details are included:\n",
    "- Name of the file.\n",
    "- The recording sound ID.\n",
    "- The start time of the slice in the original audio.\n",
    "- The end time of the slice in the original audio.\n",
    "- salience, an indicator for whether the sound slice is a foreground sound (1) or background sound (2).\n",
    "- ID of the class to which the sound slice belongs.\n",
    "- Name of the class to which the sound slice belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>0.317551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class    length  \n",
       "0          dog_bark  0.317551  \n",
       "1  children_playing  4.000000  \n",
       "2  children_playing  4.000000  \n",
       "3  children_playing  4.000000  \n",
       "4  children_playing  4.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add column calculating the length of the clips\n",
    "metadata['length'] = metadata['end'] - metadata['start']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label map of the different sound classes:\n",
    "label_map = {\n",
    "    'air_conditioner': 0,\n",
    "    'car_horn': 1,\n",
    "    'children_playing': 2,\n",
    "    'dog_bark': 3,\n",
    "    'drilling': 4,\n",
    "    'engine_idling': 5,\n",
    "    'gun_shot': 6,\n",
    "    'jackhammer': 7,\n",
    "    'siren': 8,\n",
    "    'street_music': 9,\n",
    "}\n",
    "\n",
    "def get_class_name(idx):\n",
    "    return list(label_map.keys())[list(label_map.values()).index(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the folders \n",
    "# Array of paths to each audio folder\n",
    "AUDIO_DIR = './audio' # path to audio files directory\n",
    "folders = ['fold{}'.format(x) for x in range(1, 12)] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting fold1...done!\n",
      "collecting fold2...done!\n",
      "collecting fold3...done!\n",
      "collecting fold4...done!\n",
      "collecting fold5...done!\n",
      "collecting fold6...done!\n",
      "collecting fold7...done!\n",
      "collecting fold8...done!\n",
      "collecting fold9...done!\n",
      "collecting fold10...done!\n",
      "collecting fold11...done!\n"
     ]
    }
   ],
   "source": [
    "for fold in folders:\n",
    "    print('collecting {}...'.format(fold), end = \"\")\n",
    "    files = librosa.util.find_files('{}/{}'.format(AUDIO_DIR, fold), ext=['wav'])\n",
    "    files = np.asarray(files)\n",
    "    for file in files:\n",
    "        if '.wav' in file:\n",
    "            name = file.split('/').pop()\n",
    "            idx = list(df.index[df['slice_file_name'] == name])[0]\n",
    "    print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def rename_wav_files(directory):\n",
    "#     \"\"\"\n",
    "#     Renames .wav files in the specified directory by chopping the first 4 digits,\n",
    "#     the last 2 digits, and appending an incremental counter.\n",
    "    \n",
    "#     Args:\n",
    "#         directory (str): Path to the directory containing .wav files.\n",
    "#     \"\"\"\n",
    "#     counter = 0  # Initialize the counter\n",
    "    \n",
    "#     # Iterate through all files in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith(\".wav\"):  # Process only .wav files\n",
    "#             # Chop the filename as specified\n",
    "#             parts = filename.split(\"-\")\n",
    "#             if len(parts) >= 4:\n",
    "#                 new_name = f\"{counter}-{parts[1]}.wav\"\n",
    "#                 old_path = os.path.join(directory, filename)\n",
    "#                 new_path = os.path.join(directory, new_name)\n",
    "                \n",
    "#                 # Rename the file\n",
    "#                 os.rename(old_path, new_path)\n",
    "#                 print(f\"Renamed: {filename} -> {new_name}\")\n",
    "                \n",
    "#                 # Increment the counter\n",
    "#                 counter += 1\n",
    "\n",
    "# # Specify the directory containing your .wav files\n",
    "# # directory = \"audio/fold1\"  # Replace with the actual path\n",
    "# counter = 3\n",
    "# for fold in folders:\n",
    "#     directory = f\"audio/fold{counter}\"\n",
    "#     counter += 1\n",
    "#     rename_wav_files(directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files renamed to be more useable for my purposes. \n",
    "Going to redo the function to loop through the folders. I think what I need to do is completely rewrite the function below so that it inputs the file, processes  the features,then adds to a CSV and then continues. Hopefully this will then get around the ram issues I was having. GOing to write the processing functions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8732\n",
      "8732\n"
     ]
    }
   ],
   "source": [
    "# Initialise some slices and classes arrays.\n",
    "\n",
    "slices = np.zeros(df.shape[0], dtype=object)\n",
    "slices[:] = np.nan # placeholder array for audio slices\n",
    "classes = np.array(metadata['class']) # list of audio classes for each of the audio in order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fold in folders:  \n",
    "    # Loop through each folder in the 'folders' list.\n",
    "    \n",
    "    print('collecting {}...'.format(fold), end=\"\")  \n",
    "    # Print a message indicating the folder being processed, without a new line.\n",
    "    \n",
    "    files = librosa.util.find_files('{}/{}'.format(AUDIO_DIR, fold), ext=['wav'])  \n",
    "    # Find all `.wav` files in the specified folder (inside AUDIO_DIR).\n",
    "    # Returns a list of full file paths.\n",
    "    \n",
    "    files = np.asarray(files)  \n",
    "    # Convert the list of file paths to a NumPy array, possibly for compatibility with downstream operations.\n",
    "\n",
    "    for file in files:  \n",
    "        # Loop through each `.wav` file found in the current folder.\n",
    "\n",
    "        if '.wav' in file:  \n",
    "            # Check if the file has the `.wav` extension (redundant here, as librosa already filters for it).\n",
    "\n",
    "            name = file.split('/').pop()  \n",
    "            # Extract the file name by splitting the file path and taking the last element (the name).\n",
    "\n",
    "            wave_arr, sr = librosa.load(file, sr=SAMPLE_RATE, mono=True)  \n",
    "            # Load the audio file into `wave_arr` (waveform as a NumPy array) and `sr` (sample rate).\n",
    "            # - `sr=SAMPLE_RATE` resamples the audio to the desired sample rate.\n",
    "            # - `mono=True` converts audio to a single-channel (mono) format.\n",
    "\n",
    "            idx = list(df.index[df['slice_file_name'] == name])[0]  \n",
    "            # Locate the index in the DataFrame `df` where the 'slice_file_name' matches the file name.\n",
    "            # Assumes there is exactly one match in `df`.\n",
    "\n",
    "            slices[idx] = wave_arr.astype(object)  \n",
    "            # Store the waveform data (`wave_arr`) in the `slices` array at the corresponding index (`idx`).\n",
    "            # The `.astype(object)` ensures compatibility with the `slices` array (which is of `dtype=object`).\n",
    "\n",
    "    print(\"done!\")  \n",
    "    # Indicate the completion of processing for the current folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mani is suggesting use of an LSTM as  they interact wel with time series data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
